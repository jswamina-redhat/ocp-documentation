# Purpose

This document serves as a resource to get you started on OpenShift logging. 

It can get quite complex, but this should provide a basic breakdown of how cluster logging is set up and occurs manually in order to provide foundational knowledge for related automation tasks.

# Introduction

The major components of logging are as follows:

**Collector**: Daemonset that deploys pods to each OCP node and collects logs from the node

**Log store**: Stores log data for analysis and is the default output for the log forwarder

**Application Logs**: Container logs generated by user applications running in the cluster, except infrastructure logs

**Infrastructure Logs**: Container logs generated by infrastructure namespaces: `openshift*`, `kube*`, `default`, and `journald` messages from nodes

**Audit Logs**: Logs generated by `auditd` (the node audit system) which are stored in the `/var/log/audit/audit.log` file

In order to store logs anywhere, you first need to install the Cluster Logging Operator (CLO).

You can install the operator through the GUI or the CLI (as shown below).

# Installing the Cluster Logging Operator

Create Namespace:
```
apiVersion: v1
metadata:
  name: openshift-logging
  annotations:
    openshift.io/node-selector: ""
  labels:
    openshift.io/cluster-monitoring: "true"
```

Create OperatorGroup:
```
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-logging
  namespace: openshift-logging
spec:
  channel: "stable"
  name: cluster-logging
  source: redhat-operators
  sourceNamespace: openshift-marketplace
```
You can combine these manifests into a single `yaml` file like so:
```
apiVersion: v1
metadata:
  name: openshift-logging
  annotations:
    openshift.io/node-selector: ""
  labels:
    openshift.io/cluster-monitoring: "true"
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: cluster-logging
  namespace: openshift-logging
spec:
  targetNamespaces:
  - openshift-logging
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-logging
  namespace: openshift-logging
spec:
  channel: "stable"
  name: cluster-logging
  source: redhat-operators
  sourceNamespace: openshift-marketplace
```
And run `oc apply -f <file_name>.yaml` to apply the configuration to the cluster and install the operator.

Once installed, the CLO can be used to export logs outside of the cluster or store logs locally within the OpenShift cluster.

After the operator is installed, you can create a `ClusterLogging` custom resource (CR) to schedule logging pods and other resources necessary to support logging.

Example `ClusterLogging` CR that configures `vector` as the collector:
```
apiVersion: logging.openshift.io/v1
kind: ClusterLogging
metadata:
  name: instance
  namespace: openshift-logging
spec:
  managementState: "Managed"
  collection:
    logs:
      type: "vector"
      vector: {}
```
Apply the `ClusterLogging` CR by running `oc apply -f <file_name>.yaml`

You can also create a `ClusterLogForwarder` CR to specify which logs are collected, how they are transformed, and where they are sent to (we will go into this later).

# Local Log Storage

In order to store logs locally, you will need some kind of log storage application. This can be either Elasticsearch (installed through the Elasticsearch operator) or, more recently there’s been a shift from Elasticsearch to Loki (also installed through an operator).

## Installing the Loki Operator

```
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-operators-redhat
  labels:
    openshift.io/cluster-monitoring: "true"
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: loki-operator
  namespace: openshift-operators-redhat
spec:
  targetNamespaces:
  - openshift-operators-redhat
---
apiVersion: operators.coreos/v1alpha1
kind: Subscription
metadata:
  name: loki-operator
  namespace: openshift-operators-redhat
spec:
  channel: stable
  name: loki-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
```

## Configuring LokiStack

Once the Loki operator is installed, you will need to configure Loki:
```
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki
  namespace: openshift-logging
spec:
  size: 1x.medium
  storage:
    schemas:
    secret:
      name: logging-loki-s3		# your secret name
      type: s3
  storageClassName: lvms-vg1
  tenants:
    mode: openshift-logging
```
```
apiVersion: v1
kind: Secret
metadata:
  name: logging-loki-s3
  namespace: openshift-logging
stringData:
  access_key_id: <access_key_id>
  access_key_secret: <access_key_secret>
  bucketnames: <s3_bucket_name>
  endpoint: <s3_storage_endpoint>
```
After applying these two manifests (`oc apply -f <file_name>.yaml`), Loki will create its storage space and will be ready to accept any log coming from the logging operator and store them on the storage space.

# External Log Forwarding

In order to forward logs to a third party entity, you will need to create a manifest known as the `ClusterLogForwarder`.

This manifest has three main pieces:

**Inputs**: What is the source of the logs?

**Outputs**: Where do the logs need to go?

**Pipelines**: Connects inputs and outputs and defines simple routing from one log type to one or more outputs, or which logs you want to send (application, infrastructure, or audit).

Example `ClusterLogForwarder`:
```
apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance		                    # name must be 'instance'
  namespace: openshift-logging          # ns must be 'openshift-logging'
spec:
  outputs:
  - name: loki
    type: "loki"
    url: <url>
  inputs:
  - name: my-app-logs
    application:			                   # filter application logs from 'my-project' ns
      namespaces:
      - my-project
  pipelines:
  - name: my-app
    inputRefs:
    - my-app-logs                         # Config to send logs from 'my-project' ns to loki
    outputRefs:
    - default
```

# Summary

OpenShift logging is done through the OpenShift Cluster Logging Operator (CLO)

You can store logs locally or forward them to an external log store.

The two primary resource manifests you will be writing for logging are:
- `ClusterLogging`
- `ClusterLogForwarder`

# Conclusion

Ideally you now have a foundational understanding of OpenShift logging. As mentioned earlier, logging can get quite complex, so don’t feel overwhelmed if things get hairy.

As always, don’t hesitate to reach out to the Red Hat team with any questions you may have!

## Relevant Documentation
https://docs.redhat.com/en/documentation/openshift_container_platform/4.9/html/logging/cluster-logging#cluster-logging

https://docs.redhat.com/en/documentation/openshift_container_platform/4.12/html/logging/cluster-logging#cluster-logging

https://docs.redhat.com/en/documentation/openshift_container_platform/4.12/html/logging/cluster-logging-deploying#cluster-logging-deploy-console_cluster-logging-deploying

https://docs.redhat.com/en/documentation/openshift_container_platform/4.12/html/logging/log-collection-and-forwarding#log-forwarding

https://docs.openshift.com/container-platform/4.16/observability/logging/logging-6.0/log6x-loki.html
